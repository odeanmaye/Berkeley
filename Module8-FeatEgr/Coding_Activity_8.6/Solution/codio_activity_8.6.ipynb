{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f4f3ef0fa8eec903",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Codio Activity 8.6: Using Validation to Select the Best Combination of Parameters\n",
    "\n",
    "**Expected Time: 60 Minutes**\n",
    "\n",
    "**Total Points: 30**\n",
    "\n",
    "This activity focuses on using a train/test split to select the best hyperparameters for a linear regression model complexity.  You will become familiar with scikit-learn's `train_test_split` function to generate a train/test split, and use the results to evaluate the appropriate model complexity.  The datasets used are synthetic so as to allow a comparison with the learned best complexity to that which generated the data.  \n",
    "\n",
    "## Index:\n",
    "\n",
    "- [Problem 1](#Problem-1)\n",
    "- [Problem 2](#Problem-2)\n",
    "- [Problem 3](#Problem-3)\n",
    "- [Problem 4](#Problem-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4816d0d701347e37",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Three Synthetic Datasets\n",
    "\n",
    "Below, polynomial functions of different degrees were created and noise is added to generate three basic synthetic datasets.  The relationships are then plotted. They are of varying true complexity -- cubic, quadratic, and quintic (polynomials of degree 5).  Your goal is to use cross validation to determine the appropriate model and examine its mean squared error on a set of validation data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/synthetic_8.6.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y1</th>\n",
       "      <th>y2</th>\n",
       "      <th>y3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-2.000000</td>\n",
       "      <td>-0.522368</td>\n",
       "      <td>5.698300</td>\n",
       "      <td>3.880352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.929293</td>\n",
       "      <td>-0.711336</td>\n",
       "      <td>-0.257942</td>\n",
       "      <td>8.643553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.858586</td>\n",
       "      <td>-4.759917</td>\n",
       "      <td>12.775233</td>\n",
       "      <td>6.116844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.787879</td>\n",
       "      <td>-10.255472</td>\n",
       "      <td>22.140157</td>\n",
       "      <td>12.493956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.717172</td>\n",
       "      <td>-3.503845</td>\n",
       "      <td>27.656110</td>\n",
       "      <td>10.335220</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          x         y1         y2         y3\n",
       "0 -2.000000  -0.522368   5.698300   3.880352\n",
       "1 -1.929293  -0.711336  -0.257942   8.643553\n",
       "2 -1.858586  -4.759917  12.775233   6.116844\n",
       "3 -1.787879 -10.255472  22.140157  12.493956\n",
       "4 -1.717172  -3.503845  27.656110  10.335220"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plots of the Synthetic Datasets**\n",
    "\n",
    "<img src = 'images/quad.png'/><img src = 'images/quintic.png'/><img src = 'images/cubic.png'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-51eae9cb036d0cf0",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "[Back to top](#Index:) \n",
    "\n",
    "### Problem 1\n",
    "\n",
    "#### Creating the Train and Test sets\n",
    "\n",
    "**5 Points**\n",
    "\n",
    "The scikit-learn library has a built-in function called `train_test_split` that accepts one or many arrays and returns a randomized split of the data.  Use the `train_test_split` function to split `x` and `y1` into train and test sets.  Set `random_state = 32` and create a test set using 30% of the data.  Assign your results as array's to `X_train, X_test, y1_train, y1_test` below.  \n",
    "\n",
    "- In anticipation of using `LinearRegression` estimator, make sure your `X_train` and `X_test` are of shapes (70, 1) and (30, 1) respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9434424b94dba3c0",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100,) (70, 1) (30, 1)\n",
      "           x\n",
      "91  4.434343\n",
      "21 -0.515152\n",
      "61  2.313131\n",
      "0  -2.000000\n",
      "31  0.191919\n"
     ]
    }
   ],
   "source": [
    "### GRADED\n",
    "\n",
    "X_train, X_test, y1_train, y1_test = '', '', '', ''\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "X_train, X_test, y1_train, y1_test = train_test_split(df[['x']], df['y1'], random_state = 32, test_size=.3)\n",
    "### END SOLUTION\n",
    "\n",
    "# Answer check\n",
    "print(df['x'].shape, X_train.shape, X_test.shape)\n",
    "print(X_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-1272c02c9402c860",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN HIDDEN TESTS\n",
    "X_train_, X_test_, y1_train_, y1_test_ = train_test_split(df[['x']], df['y1'], random_state = 32, test_size=.3)\n",
    "#\n",
    "#\n",
    "#\n",
    "assert (X_train.shape) == X_train_.shape, \"Make sure X_train is (70, 1) not (70,) -- keep it as a DataFrame when passing to train_test_split with df[['x']]\"\n",
    "np.testing.assert_array_equal(X_train, X_train_, err_msg='Make sure to set the random state and test_size argument')\n",
    "np.testing.assert_array_equal(y1_train, y1_train_, err_msg=\"Don't forget to include y1 -- this should be shape (70,)\")\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-de81ef2bbaab88ac",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "[Back to top](#Index:) \n",
    "\n",
    "### Problem 2\n",
    "\n",
    "**5 Points**\n",
    "\n",
    "Use the `train_test_split` function to create similar splits of `y2` and `y3`.  Use the `random_state = 32` and create a test set using 30% of the data.   Assign your results as  to `y2_train`, `y2_test`, `y3_train`, and `y3_test` below.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-14a80d1dd8e68ebc",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100,) (70,) (30,)\n",
      "91   -173.744136\n",
      "21    -28.021552\n",
      "61    -15.806732\n",
      "0       5.698300\n",
      "31    -21.696552\n",
      "Name: y2, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "### GRADED\n",
    "\n",
    "y2_train, y2_test = '', ''\n",
    "y3_train, y3_test = '', ''\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "y2_train, y2_test, y3_train, y3_test = train_test_split(df['y2'], df['y3'], random_state = 32, test_size=.3)\n",
    "### END SOLUTION\n",
    "\n",
    "# Answer check\n",
    "print(df['y2'].shape, y2_train.shape, y2_test.shape)\n",
    "print(y2_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-655e8461ffcd9379",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN HIDDEN TESTS\n",
    "y2_train_, y2_test_, y3_train_, y3_test_ = train_test_split(df['y2'], df['y3'], random_state = 32, test_size=.3)\n",
    "#\n",
    "#\n",
    "#\n",
    "assert y2_train.shape == y2_train_.shape, \"Make sure y_train is (70,) not (70,1)\"\n",
    "np.testing.assert_array_equal(y2_train, y2_train_, err_msg='Make sure to set the random state and test_size argument')\n",
    "np.testing.assert_array_equal(y3_train, y3_train_, err_msg=\"Don't forget to include y3_train -- this should be shape (70,)\")\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d22e18e493c7a842",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "[Back to top](#Index:) \n",
    "\n",
    "### Problem 3\n",
    "\n",
    "**10 Points**\n",
    "\n",
    "Use a `for` loop to loop over the values from one to 20. For each iteration `i`:\n",
    "\n",
    "- Use `Pipeline` to create a pipeline object. Inside the pipeline object define a a tuple where the first element is a string identifier `pfeat` and the second element is an instance of `PolynomialFeatures` of degree `i` with `include_bias = False`. Inside the pipeline define another tuple where the first element is a string identifier `linreg`, and the second element is an instance of `LinearRegression`. Assign the pipeline object to the variable `pipe`.\n",
    "- Use the `fit` function on `pipe` to train your model on `X_train` and `y1_train`. Assign the result to `train_preds`.\n",
    "- Use the `predict` function on `pipe` to compute your prediction on `X_test`. Assign the result to `test_preds`.\n",
    "- Use the `mean_squared_error` function to calculate the MSE between `y1_train` and `train_preds`. Append your result to the `train_mses` list.\n",
    "- Use the `mean_squared_error` function to calculate the MSE between `y1_test` and `test_preds`. Append your result to the `test_mses` list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-22388d4a41b01c98",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "### GRADED\n",
    "\n",
    "train_mses = []\n",
    "test_mses = []\n",
    "\n",
    "#for complexity 1 - 20:\n",
    "\n",
    "    #create pipeline with PolynomialFeatures and LinearRegression\n",
    "    #remember to set include_bias = False\n",
    "    \n",
    "    #fit pipeline on training data\n",
    "    \n",
    "    #mse of training data\n",
    "    \n",
    "    #mse of testing data\n",
    "\n",
    "best_model_complexity = ''\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "train_mses = []\n",
    "test_mses = []\n",
    "for i in range(1, 21):\n",
    "    pipe = Pipeline([('pfeat', PolynomialFeatures(degree = i, include_bias=False)), \n",
    "                    ('linreg', LinearRegression())])\n",
    "    pipe.fit(X_train, y1_train)\n",
    "    train_preds = pipe.predict(X_train)\n",
    "    test_preds = pipe.predict(X_test)\n",
    "    train_mses.append(mean_squared_error(y1_train, train_preds))\n",
    "    test_mses.append(mean_squared_error(y1_test, test_preds))\n",
    "    \n",
    "best_model_complexity = test_mses.index(min(test_mses)) + 1\n",
    "### END SOLUTION\n",
    "\n",
    "# Answer check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-e0ff7e648cd25c41",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN HIDDEN TESTS\n",
    "train_mses_ = []\n",
    "test_mses_ = []\n",
    "for i in range(1, 21):\n",
    "    pipe = Pipeline([('pfeat', PolynomialFeatures(degree = i, include_bias=False)), \n",
    "                    ('linreg', LinearRegression())])\n",
    "    pipe.fit(X_train_, y1_train_)\n",
    "    train_preds_ = pipe.predict(X_train_)\n",
    "    test_preds_ = pipe.predict(X_test_)\n",
    "    train_mses_.append(mean_squared_error(y1_train_, train_preds_))\n",
    "    test_mses_.append(mean_squared_error(y1_test_, test_preds_))\n",
    "best_model_complexity_ = test_mses_.index(min(test_mses_)) + 1\n",
    "#\n",
    "#\n",
    "#\n",
    "# Compare variables using assert\n",
    "assert len(test_mses) == len(test_mses_), 'Make sure you try degree 1 - 20.'\n",
    "assert test_mses_ == test_mses, 'Check that you evaluate MSE on the test data'\n",
    "assert train_mses_ == train_mses, 'Check your training MSE computation'\n",
    "assert best_model_complexity == best_model_complexity_\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment the code below to visualize the results of your model fitting.  Note that the data in `y1` were created from a quadratic model originally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f'The Complexity that minimized Test Error was: {test_mses.index(min(test_mses)) + 1}')\n",
    "# plt.plot(range(1, 21), train_mses, '--o', label = 'training error')\n",
    "# plt.plot(range(1, 21), test_mses, '--o', label = 'testing error')\n",
    "# plt.xticks(range(1, 21), range(1, 21))\n",
    "# plt.xlabel('Degree Complexity')\n",
    "# plt.ylabel('Mean Squared Error')\n",
    "# plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-fbff2877fa3e10c1",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "[Back to top](#Index:) \n",
    "\n",
    "### Problem 4\n",
    "\n",
    "#### Write a function to determine best model complexity\n",
    "\n",
    "**10 Points**\n",
    "\n",
    "Complete the definition of the `simple_cross_validation` function according to the following instructions:\n",
    "\n",
    "\n",
    "Use a `for` loop to loop over the values from one to 20. For each iteration `i`:\n",
    "\n",
    "- Use `Pipeline` to create a pipeline object. Inside the pipeline object define a a tuple where the first element is a string identifier `pfeat` and the second element is an instance of `PolynomialFeatures` of degree `i` with `include_bias = False`. Inside the pipeline define another tuple where the first element is a string identifier `linreg`, and the second element is an instance of `LinearRegression`. Assign the pipeline object to the variable `pipe`.\n",
    "- Use the `fit` function on `pipe` to train your model on `X_train` and `y_train`. \n",
    "- Use the `predict` function on `pipe` to compute your prediction on `X_test`. Assign the result to `test_preds`.\n",
    "- Use the `mean_squared_error` function to calculate the MSE between `y_test` and `test_preds`. Assign your result to `test_mse`.\n",
    "- Use an `if` statement to check that `test_mse` is less than `best_mse`:\n",
    "    - If the condition is satisifed assign `test_mse` to `best_mse` and `pipe` to `best_pipe`.\n",
    "- Your function should return `best_pipe`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-68f0fca2c0aa71b4",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('pfeat', PolynomialFeatures(degree=10, include_bias=False)),\n",
       "  ('linreg', LinearRegression())],\n",
       " 'verbose': False,\n",
       " 'pfeat': PolynomialFeatures(degree=10, include_bias=False),\n",
       " 'linreg': LinearRegression(),\n",
       " 'pfeat__degree': 10,\n",
       " 'pfeat__include_bias': False,\n",
       " 'pfeat__interaction_only': False,\n",
       " 'pfeat__order': 'C',\n",
       " 'linreg__copy_X': True,\n",
       " 'linreg__fit_intercept': True,\n",
       " 'linreg__n_jobs': None,\n",
       " 'linreg__normalize': 'deprecated',\n",
       " 'linreg__positive': False}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### GRADED\n",
    "\n",
    "def simple_cross_validation(X_train, y_train, X_test, y_test):\n",
    "    best_pipe = None #placeholder for best model\n",
    "    best_mse = np.inf #set best mse to infinity to begin\n",
    "    #for complexity 1 - 20:\n",
    "\n",
    "        #create pipeline with PolynomialFeatures and LinearRegression\n",
    "        #remember to set include_bias = False\n",
    "\n",
    "        #fit pipeline on training data\n",
    "\n",
    "        #mse of testing data\n",
    "        \n",
    "        #if mse is best -- set best_pipe = pipe\n",
    "        \n",
    "        #return best pipeline\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "def simple_cross_validation(X_train, y_train, X_test, y_test):\n",
    "    best_pipe = None #placeholder for best model\n",
    "    best_mse = np.inf #set best mse to infinity to begin\n",
    "    for i in range(1, 21):\n",
    "        pipe = Pipeline([('pfeat', PolynomialFeatures(degree = i, include_bias=False)), \n",
    "                    ('linreg', LinearRegression())])\n",
    "        pipe.fit(X_train, y_train)\n",
    "        test_preds = pipe.predict(X_test)\n",
    "        test_mse = mean_squared_error(y_test, test_preds)\n",
    "        if test_mse < best_mse:\n",
    "            best_mse = test_mse\n",
    "            best_pipe = pipe\n",
    "    return best_pipe\n",
    "### END SOLUTION\n",
    "\n",
    "best_model = simple_cross_validation(X_train, y2_train, X_test, y2_test)\n",
    "best_model.get_params() #should be degree = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-81aaadd1a6b5495b",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "### BEGIN HIDDEN TESTS\n",
    "# Answer computation\n",
    "def simple_cross_validation_(X_train, y_train, X_test, y_test):\n",
    "    best_pipe = None \n",
    "    best_mse = np.inf \n",
    "    for i in range(1, 21):\n",
    "        pipe = Pipeline([('pfeat', PolynomialFeatures(degree = i, include_bias=False)), \n",
    "                    ('linreg', LinearRegression())])\n",
    "        pipe.fit(X_train, y_train)\n",
    "        test_preds = pipe.predict(X_test)\n",
    "        test_mse = mean_squared_error(y_test, test_preds)\n",
    "        if test_mse < best_mse:\n",
    "            best_mse = test_mse\n",
    "            best_pipe = pipe\n",
    "    return best_pipe\n",
    "\n",
    "ans1 = simple_cross_validation(X_train, y1_train, X_test, y1_test)\n",
    "ans1_ = simple_cross_validation_(X_train, y1_train, X_test, y1_test)\n",
    "ans2 = simple_cross_validation(X_train, y3_train, X_test, y3_test)\n",
    "ans2_ = simple_cross_validation_(X_train, y3_train, X_test, y3_test)\n",
    "#\n",
    "#\n",
    "#\n",
    "assert list(ans1.get_params().values())[-9] == list(ans1_.get_params().values())[-9], 'Check the degree of your best model with y1'\n",
    "assert list(ans2.get_params().values())[-9] == list(ans2_.get_params().values())[-9], 'Check the degree of your best model with y3'\n",
    "### END HIDDEN TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
