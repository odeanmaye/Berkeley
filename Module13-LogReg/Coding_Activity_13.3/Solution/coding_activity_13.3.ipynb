{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c4ad417ef9c632ba",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Codio Activity 13.3: Determining the Logistic Functions Parameters\n",
    "\n",
    "**Expected Time = 60 minutes** \n",
    "\n",
    "**Total Points = 50** \n",
    "\n",
    "This activity focuses on determining the appropriate parameters for the logistic model using optimization.  To begin, you will write a function to represent $\\sigma(x)$.  Then, you will use the scikit-learn metric `log_loss` to evaluate the cross entropy based on the probabilities.  Finally, you will explore different parameters to select that which minimizes the cross entropy.\n",
    "\n",
    "#### Index\n",
    "\n",
    "- [Problem 1](#-Problem-1)\n",
    "- [Problem 2](#-Problem-2)\n",
    "- [Problem 3](#-Problem-3)\n",
    "- [Problem 4](#-Problem-4)\n",
    "- [Problem 5](#-Problem-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "penguins = sns.load_dataset('penguins').dropna()\n",
    "penguins = penguins.loc[(penguins['species'] == 'Adelie') | (penguins['species'] == 'Gentoo')]\n",
    "X = penguins.drop('species', axis = 1)[['flipper_length_mm']]\n",
    "y = np.where(penguins.species == 'Adelie', 0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Problem\n",
    "\n",
    "As discussed in the lectures, under Gaussian assumptions of the data distributions, you are able to directly compute the parameters for the logistic model.  However, this is not going to work in higher dimensional settings.  As how in the figure below, this assumption seems reasonable for the flipper length of our penguins data.\n",
    "\n",
    "<center>\n",
    "    <img src = 'images/flipperdist.png' />\n",
    "</center>\n",
    "\n",
    "However, rather than using the mean and variance of these distributions learned through maximum likelihood, we will frame the problem of parameter estimation as one of minimization, and among a range of possible $\\beta$ values, select that which minimized cross entropy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e1e88c4555a6b649",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "[Back to top](#-Index)\n",
    "\n",
    "### Problem 1\n",
    "\n",
    "#### Sigma Function\n",
    "\n",
    "**10 Points**\n",
    "\n",
    "Complete the definition of the `sigma` function  below that takes in an array `x`, `beta_0`, `beta_1`. this function should return the evaluation of the Sigma function according to the formula:\n",
    "\n",
    "$$\\sigma(x) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1x)}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-533b33420380568c",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     flipper_length_mm\n",
      "0                  1.0\n",
      "1                  1.0\n",
      "2                  1.0\n",
      "4                  1.0\n",
      "5                  1.0\n",
      "..                 ...\n",
      "338                1.0\n",
      "340                1.0\n",
      "341                1.0\n",
      "342                1.0\n",
      "343                1.0\n",
      "\n",
      "[265 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "### GRADED\n",
    "\n",
    "def sigma(x, beta_0, beta_1):\n",
    "    \"\"\"\n",
    "    This function evaluates the sigmoid function with \n",
    "    given parameters beta_0 and beta_1\n",
    "    \n",
    "    Argments\n",
    "    --------\n",
    "    x: np.array\n",
    "       domain for evaluation of sigma\n",
    "    beta_0: float\n",
    "       intercept term of linear function\n",
    "    beta_1: float\n",
    "        slope of linear term\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    np.array \n",
    "    \"\"\"\n",
    "    return None\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "def sigma(x, beta_0, beta_1):\n",
    "    zi = beta_0 + beta_1*x\n",
    "    return 1/(1 + np.exp(-zi))\n",
    "### END SOLUTION\n",
    "\n",
    "# Answer check\n",
    "print(sigma(X, 1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2dedf86070c9d2da",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "[Back to top](#-Index)\n",
    "\n",
    "### Problem 2\n",
    "\n",
    "#### Using a given $\\beta_0$ and $\\beta_1$ for predictions\n",
    "\n",
    "**10 Points**\n",
    "\n",
    "The `sigma` function should return probabilities for making a classification.  \n",
    "\n",
    "Use the `sigma` function defined in problem 1 and and the given `beta_0` and `beta_1` below to create predictions on the array `X`. Assign this result to the `probs` variable.\n",
    "\n",
    "\n",
    "\n",
    "Next, if the value inside `probs` is greater than or equal to `.5` predict class `1`, otherwise predict `0`. Assign subsequent predictions based on these probabilities to the `predictions` variable.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_0 = -165\n",
    "beta_1 = 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8086826f4d0f39db",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "### GRADED\n",
    "probs = ''\n",
    "predictions = ''\n",
    "\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "probs = sigma(X, beta_0, beta_1)\n",
    "predictions = np.where(probs >= 0.5, 1, 0)\n",
    "### END SOLUTION\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6810d9f56a3486e3",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "[Back to top](#-Index)\n",
    "\n",
    "### Problem 3\n",
    "\n",
    "#### Computing the Cross Entropy\n",
    "\n",
    "**10 Points**\n",
    "\n",
    "Recall that our goal is to find the vales of `beta_0` and `beta_1` that minimize the cross entropy give by:\n",
    "\n",
    "$$-\\sum_{i = 1} ^ N \\big( (1 - y_i)\\log(1 - \\sigma(x_i)\\big) + \\big(y_i(\\log \\sigma(x_i) \\big)$$\n",
    "\n",
    "Scikit-learn has an implementation of the cross entropy with the `log_loss` function imported below.\n",
    "\n",
    "\n",
    "To use this, we want to pass an array of probabilities for both the positive and negative classes against the true `y` values.  Below, this array is created for you using the earlier probabilities. \n",
    "\n",
    "\n",
    "In the code cell below, use the `log_loss` function on `y` and `prob_array` to conpute the cross entropy. Assign the value to `loss1` below.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>probability 0</th>\n",
       "      <th>probability 1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.325527e-17</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.701087e-16</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.193796e-13</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.034177e-13</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.266417e-14</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   probability 0  probability 1\n",
       "0   2.325527e-17            1.0\n",
       "1   7.701087e-16            1.0\n",
       "2   4.193796e-13            1.0\n",
       "3   1.034177e-13            1.0\n",
       "4   1.266417e-14            1.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_array = np.concatenate((probs.values, (1 - probs).values), axis = 1)\n",
    "pd.DataFrame(prob_array, columns = ['probability 0', 'probability 1']).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4a2f889191ea3f75",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.157337553379403\n"
     ]
    }
   ],
   "source": [
    "### GRADED\n",
    "\n",
    "loss1 = ''\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "loss1 = log_loss(y, prob_array)\n",
    "### END SOLUTION\n",
    "\n",
    "# Answer check\n",
    "print(loss1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2b7720b972f0e0e5",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "[Back to top](#-Index)\n",
    "\n",
    "### Problem 4\n",
    "\n",
    "#### Comparing Loss across Parameters\n",
    "\n",
    "**10 Points**\n",
    "\n",
    "Remember that your goal is to identify the parameters that **minimize** the log loss.  However, we know the rough values of this from our last assignment -- at least the values that come from the solved optimization problem with Scikit-learn.  \n",
    "\n",
    "Accordingly, consider an array of parameters `beta_1s`.  Complete the code below to loop over `beta_1s`. Inside the `for` loop, use the `sigma` function to evaulate the probabilies. Assign this result to `probs`. Next, use the NumPy function `concatenate` to pass an array of probabilities for both the positive and negative classes against the true `y` values. Assign this result to ` prob_array`. Finally, use the `log_loss` function to compute the cross entropy loss of true `y` against `prob_array`. Store this result  as a list in the `losses` variable.  \n",
    "\n",
    "Finally, determine the `beta_1` that minimizes the `log_loss` and assign it as a float to `best_beta1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_1s = np.linspace(0, 1, 1000) #array of beta_1's to use\n",
    "beta_0 = -142 #beta_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4777ea1367c2910f",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7087087087087087\n"
     ]
    }
   ],
   "source": [
    "### GRADED\n",
    "losses = []\n",
    "for beta_1 in beta_1s:\n",
    "    probs = ''\n",
    "    prob_array = ''\n",
    "    losses.append(None)\n",
    "    \n",
    "best_beta1 = ''\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "losses = []\n",
    "for beta_1 in beta_1s:\n",
    "    probs = sigma(X, beta_0, beta_1)\n",
    "    prob_array = np.concatenate((probs.values, (1 - probs).values), axis = 1)\n",
    "    losses.append(log_loss(y, prob_array))\n",
    "    \n",
    "best_beta1 = beta_1s[losses.index(min(losses))]\n",
    "### END SOLUTION\n",
    "\n",
    "# Answer check\n",
    "print(best_beta1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-bf9a48b3efcd7682",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "[Back to top](#-Index)\n",
    "\n",
    "### Problem 5\n",
    "\n",
    "#### Comparing the results to `LogisticRegression`\n",
    "\n",
    "**10 Points**\n",
    "\n",
    "In reality, a solver that implements gradient descent is used to target the optimal parameter values.  Specifically, the solvers `'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'` are available and `liblinear` is the default.  \n",
    "\n",
    "As a check, implement a `LogisticRegression` estimator with all default settings and examine the coefficient.  Is it close to what you selected in Problem 4 above?  \n",
    "\n",
    "Assign your fit estimator as `log_reg` the coefficient as a float to `coef` and the absolute difference between the coefficient from scikitlearn and that of `best_beta1` from Problem 4 as difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-39462d43514bd0d8",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beta_1 from sklearn: 0.6948193765225441\n",
      "Beta_1 from our minimization: 0.7087087087087087\n",
      "Difference:  0.01\n"
     ]
    }
   ],
   "source": [
    "### GRADED\n",
    "\n",
    "logreg = ''\n",
    "coef = ''\n",
    "difference = ''\n",
    "\n",
    "### BEGIN SOLUTION\n",
    "logreg = LogisticRegression().fit(X, y)\n",
    "coef = float(logreg.coef_)\n",
    "difference = abs(coef - best_beta1)\n",
    "### END SOLUTION\n",
    "\n",
    "# Answer check\n",
    "print(f'Beta_1 from sklearn: {coef}\\nBeta_1 from our minimization: {best_beta1}\\nDifference: {difference: .2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
